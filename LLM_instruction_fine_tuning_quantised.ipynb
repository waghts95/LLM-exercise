{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFMc92dpN4G4"
   },
   "source": [
    "This is low memory training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAxzv_OovPD6"
   },
   "source": [
    "üß± 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3q5QNYctWSQ",
    "outputId": "1db62cac-bdb3-48c3-9afb-fa69c6ae6ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m198 packages\u001b[0m \u001b[2min 70ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m187 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv add transformers datasets accelerate bitsandbytes peft sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QItwdrb8vaVy"
   },
   "source": [
    "üß† 2Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2EL6kTv7vZnO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushar/work/LLM/RAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGWqZ8sb67FJ"
   },
   "source": [
    "üß© 3Ô∏è‚É£ Inspect and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data && gsutil -m cp gs://tusharwagh.appspot.com/data/combined_dataset.json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "900b4366e7b3485d9f5f6f5b34a48385",
      "78f6ea6fca7b4f48af3964dc58a4bd9d",
      "a8121390f11740aeb6ac89c770a290df",
      "178887ac16bc4ec3adb2de6e2b314a4c",
      "97570ff4a7814c9594f139d1836cc29f",
      "c36378e7e70b4cd482954aece95282b9",
      "a0a95d223b394efb9ce7efc77725847c",
      "10d9251f653e4f84a698b42cc29cd7ea",
      "075d43f75317412eb4e6c71b07ee885e",
      "a4fd8fe24d4746db871068fd6702fefb",
      "9846068fc78f4a4aa4e4f9a6e7f80b2b"
     ]
    },
    "id": "baR0CGjp8n6-",
    "outputId": "641ba1a6-b127-4914-bf4f-a5d9f38a41ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Context', 'Response'],\n",
      "        num_rows: 3512\n",
      "    })\n",
      "})\n",
      "{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your uploaded JSON file directly\n",
    "dataset = load_dataset(\"json\", data_files=\"data/combined_dataset.json\")\n",
    "\n",
    "# View sample\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfdhBgfj8y_E"
   },
   "source": [
    "üß© Prepare for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM8RYFrI858Y"
   },
   "source": [
    "Combine context and response into one conversational string so the model learns counselor-style replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c58f045ced7443abbd1799d1179a4aec",
      "f52cd766a9ed4751af32ac28460d72f4",
      "77d5c424032f4bf396a81aea9fefc026",
      "785f065fd5bf4ea2aa7fb15b58e53a86",
      "b642d65875f74ca8869eef9c2c3b430a",
      "e89f2ca47dcb49e3942fde9bc98f1d5a",
      "fda85b2a0f854ddfbee3bcbdd11f11fd",
      "c52acc563f9046e5922e81b9a5fc4e8f",
      "18690b76fdcd48d48838865f318d5ea8",
      "c8e0a001db124a3799c681351fd49ee9",
      "9446536cada94e0db86ebb0b447eb67f"
     ]
    },
    "id": "ytQFIiFy82P6",
    "outputId": "a490815b-da72-488a-f0c8-dce9027906aa"
   },
   "outputs": [],
   "source": [
    "def format_conversation(example):\n",
    "    return {\n",
    "        \"text\": f\"Client: {example['Context'].strip()}\\nCounselor: {example['Response'].strip()}\"\n",
    "    }\n",
    "\n",
    "dataset = dataset[\"train\"].map(format_conversation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxdDDSXjwJ90"
   },
   "source": [
    "ü¶ô 6Ô∏è‚É£ Choose Base Llama Model\n",
    "\n",
    "You can choose any open-source variant:\n",
    "\n",
    "    Model\t                         Parameter\t   Notes<br>\n",
    "    \"meta-llama/Meta-Llama-3-8B\"\t  8B\t      Best balance of quality/performance\n",
    "    \"meta-llama/Llama-2-7b-hf\"\t      7B\t      Older but lightweight\n",
    "    \"meta-llama/Meta-Llama-3-70B\"\t  70B\t      For multi-GPU clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MFUUfbswqtR"
   },
   "source": [
    "For Colab, stick with the 8B or 7B versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_q60gKszjuz"
   },
   "source": [
    "‚öôÔ∏è 8Ô∏è‚É£ Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "675e69a1456e414bb98a2e1b9e019f88",
      "44d5753da82f4e519ca6c63505890d3e",
      "ae2b0f90320a4bd69f07a991b1b12aae",
      "2041ed7e4c9d42c085698a96fe8352e1",
      "d957ac6b6d13472bb3b6a7eefd163ded",
      "5d75ed57ba4547c3b616ad6b1787b26f",
      "065cde6b6568468ebec4d396c49a346e",
      "625720a301134bd88c405b8ecb3b05a7",
      "9f4eb07c3d8545b4bc6d785a9aad95a5",
      "5038dc7aa69d4aed99efa1f09a157ca1",
      "c9e1df4e74b746d1bfefa6a31b5198a8",
      "6a2a6cdfae1143e3a5350bc58a44d364",
      "b978cdfa3f4f42d8a6a3b498363a57cb",
      "2bcc633a02794a28998801beb8484aca",
      "37a4ad3c6b4a4334ba04b8f48924ea17",
      "8bab348c9c7e4d108704af97a09ed78f",
      "f1d4574e060f4953afd9a70d6c06e649",
      "4cab77e9157b4b759de8c63cb0c727bb",
      "4297314214a64ae5b2e33bf154472a2d",
      "1429f31770544d7c936fa7646e7ad84b",
      "5acd17a8cc064d1bbcba84a9d236601d",
      "6873eaccff4445d3aa431b302a8f0431",
      "21a2a8a8fee2463b9d304945905d6017",
      "1de4f89821144a74b9b6478e2a797659",
      "8965a665f14241709fcdde3bb202fc76",
      "3608ef145cf540b8b8a698f6237b39ec",
      "3c597546491b4ce4a1025f13d7659c4f",
      "1e4ad6c0d08c49509db8f45106185998",
      "a7779ad16602453d846596cb1e861e71",
      "c35fa6fc14ea43b7b1c5bfaac0946216",
      "984d65afc28e41b3a601cfc2f8b488c0",
      "2a459691aa164710ab35911d6aa670fc",
      "016c8c99fc0a47178f095bc93382a892",
      "eaaf84f9ad714ef1a6f8f80667d76c40",
      "9b606f7f75244419b4ec9ea015cc5648",
      "fa71c20ae7f54a6fac170615105b677d",
      "821ba891f22c494c950e9c8651bffbdd",
      "95bc4daf99ea400e957c81fb7768f5a6",
      "249c83771c9b45d6a8d35fbebd91cd87",
      "d0bc5db089db4007b0a261e08f4b7178",
      "153eb8f2cbc34c2f916d2c0402680d8e",
      "a5901382e26b4fa68eceec08a52c61f7",
      "4ac487c3b15d4d0ca214339d3f7295b7",
      "f89a2918e39041268752852f8c9c8a56",
      "7cc9cf5e56094acb8df058e392b8e555",
      "ca0ea2f9b45a466e9ca58cca441c5f16",
      "0a7f64acce7c4fe4bbb3916fe1dc4006",
      "a4064b1a22494dff9d8344e037844fc2",
      "128c280a76354fb59448357c41b764d4",
      "3e44f8f14e2e4f4bb6d448aea97a990e",
      "873b5621f31d49f3b883c73f0dc0ced7",
      "501739f37a7b4d06a085510b7a5c50a4",
      "35a27d61082c4dedbeaf4d1b0cf621af",
      "a635924d9831414f8d428aa59132a033",
      "94df6383296e4459accc3403282df487",
      "6a3ee1c058a14de5846e0cbb21eef1a3",
      "737d2b7fc3464d2b85e02b40e06e311d",
      "5d92782bd5c64ef9bcad7a5b28f0ac5e",
      "66c91e8c87b54790a04a2143c586f91d",
      "f46810ff2194420a8808ff4256524f8c",
      "d25daa6a733d4f228b7afbcb6f12cad9",
      "343f74fe759f4ccca2e7661e644cb0a1",
      "5dc7dee8374c4531a4bd21a88a65dcb1",
      "7074f14dcce14d3b9becd7874266d2ca",
      "f6f619f61df64429ac386dd9688f42ba",
      "f5bcead9018b4ea3aa341b8a4be88b57",
      "2bc5a0304e3b47bebd54472d95bc7dec",
      "edb09401b38348b98b14a9b989dd6517",
      "fbb60a9fb52445bcbee8eecc98c6e8e0",
      "09e93ffea22545b193bdce30b57e0ff6",
      "d3d72df8334641589ff5e2ef642e3756",
      "d87b1ecaf63b4e65880e99765332550c",
      "34710298f1bc49138f408d86c57bba49",
      "1f77d6d3b4364fb9930fda3862e4f4a4",
      "56c20d19904f40658702fd8cf0ad3250",
      "b36ceba4b7c94754bd9199ed2c7d069c",
      "98e258759ec7416fad7c9cdc058dc6a0",
      "ca496a2965fb4069bbe3bc9d3b061667",
      "62d3466a9ead411294298d4348345e96",
      "0cd73cab8d234a8d9419c17a0fad69ef",
      "4b20f8eed2f749c9ba013d6b5f4ed82b",
      "c3d31a12abf645aab7edac846edf7283",
      "4f0aa394101e407989cd8223b23b2caf",
      "abe23ff5247b4930a11979792d030a51",
      "42bf5191a93140d3b408aa6040d8b13d",
      "8dcb2d3bbab8492eaaf87e4466d5ad35",
      "bb9fda24bd1f4c25a828de4271111d46",
      "9379bb2794144b54b3416a3309ea6493",
      "1d385b8c45c542c6ab5ce7475f5730ee",
      "c2ff826336264307afe916395b0acda9",
      "76fb8fd9f0674cbc8b20e927c8a51876",
      "40fd9271dfab43f7b6500eba085c16d3",
      "6076964c8bb54dd8970bb1d91a1e459f",
      "a992e458c53e403cb4ef133a92c1ab9d",
      "aca32d69b4034cff98c5201a385799cb",
      "102391092b7c4117bb687041a5265516",
      "099e29e337b2479baad7ebffca880ab5",
      "1c68b779fa0440caa702e50c422818b9",
      "336dedde1b4f42c5be1d9540465ce448",
      "8109127d4e3c48de8610215d44587e29",
      "cf217478ed4a4638994036e17367bd2d",
      "1209acaf1df4438ebebe5f82bc84db8e",
      "48b7237be9d94a68b828a8f99fa89b3a",
      "2e7d1e178c4f4cad92ded2dbe0bc788f",
      "5e1abbe256354f2f97626b576925de3d",
      "3a213f5e38e441a6a9b38ad2b954fdfc",
      "7ba8d2ab540f4d5ba3065463705cb817",
      "6607e243617e4ccdb52782b33a072e22",
      "67e3688553b243838df249553ad86df8",
      "6cf61bd57ae64166adb2141c3811211a",
      "f10d1416dff84af48372274f6d5a4991",
      "836cf3d66882407daba14cb259d9134d",
      "55cbc942bbf14d9aa91c0f5441e89a80",
      "55d4c55a342745caa36a6b24d26abd15",
      "e846fbf3691140109e3f73429e469d52",
      "8edcc3097c204f2086c14e1e43adcdae",
      "5b892c1eb16b43ba99ee120d1645701c",
      "9233a5127fde493da0f5f326df69e5e4",
      "a0a2f28897184877b15ae6ef4bac8a3b",
      "0854829a63e849a39cc01a0ef534c5bc",
      "01c2f72b82ac4389ac84e3049b779699",
      "1ac54a86d4b64b7db7a93018f0027cf3",
      "d1aeff8919a04121bf4b344a81561f5a",
      "d8fb3fca70c7413ca2ebea40f92fd343",
      "26fd0697f8014e7e962e573c767a78ac",
      "cdedbd3f038e4b8086198a0deebc7570",
      "812ca79b28f64ff2824a9f6463554083",
      "9bb4ece64978463ba800d1e381aca97f",
      "03c67fe2d48e4866acb7535017690998",
      "94c8d0a9e6a146f8b95d3bbbcf734b61",
      "49f04e12cabe4fc29d329db052e559ec",
      "8e9956f660ef43709956b6d4f8283fa9"
     ]
    },
    "id": "AjDvIwxowlvz",
    "outputId": "c7d3a3b9-bd6d-4ceb-ce65-f3c3109a125c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\" # Changed to a publicly available model\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad_token by default\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Apply quantization config\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch.float16, # Data type is handled by BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Prepare model for QLoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config for low-rank adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI9lz1Cyzeoj"
   },
   "source": [
    "üßæ 7Ô∏è‚É£ Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gtw0_gLxzgt7"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBk_2xbn-y0f"
   },
   "source": [
    "‚öôÔ∏è Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9-u4hDdMzku4"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eNhb3s2ztnn"
   },
   "source": [
    "üöÄ üîü üßÆ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uvACDsOfztAo"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_counseling_domain\",\n",
    "    per_device_train_batch_size=1,  # Reduced batch size\n",
    "    gradient_accumulation_steps=32, # Further increased gradient accumulation steps\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    "    #fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True, # Enabled gradient checkpointing\n",
    "    optim=\"adamw_torch\", # Specify AdamW optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv8uurREzyKz"
   },
   "source": [
    "üöÄ 7Ô∏è‚É£ Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "QzoA0-aIz6Xb",
    "outputId": "93cbbfd6-cc51-44e5-a72f-8ce5227741bb"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 73.31 MiB is free. Including non-PyTorch memory, this process has 5.53 GiB memory in use. Of the allocated memory 4.97 GiB is allocated by PyTorch, and 451.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[1;32m      5\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/LLM/RAG/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 73.31 MiB is free. Including non-PyTorch memory, this process has 5.53 GiB memory in use. Of the allocated memory 4.97 GiB is allocated by PyTorch, and 451.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkIH5Wk5-kbC"
   },
   "source": [
    "üíæ 8Ô∏è‚É£ Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGbgP9CM-lT7"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./llama3_counseling_domain\")\n",
    "tokenizer.save_pretrained(\"./llama3_counseling_domain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UIXRTCm-pSx"
   },
   "source": [
    "üß™ 9Ô∏è‚É£ Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOEl76jm-nzT"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"./llama3_counseling_domain\", tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Client: I feel anxious and worthless lately. What should I do?\\nCounselor:\"\n",
    "print(pipe(prompt, max_new_tokens=100)[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
