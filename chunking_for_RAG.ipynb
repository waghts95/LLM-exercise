{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJY885nmTcC5"
   },
   "source": [
    "Let’s Build a Realistic Mini Pipeline<br>\n",
    "Below is a fully working example that:\n",
    "\n",
    "\n",
    "\n",
    "    Downloads multiple large text files from the web.\n",
    "\n",
    "    Saves them locally.\n",
    "\n",
    "    Reads them back.\n",
    "\n",
    "    Chunks them efficiently (token-based).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owelx-fMTuJU"
   },
   "source": [
    "Download → Read → Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gn5eaXtnT2C9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "from typing import List, Dict, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IELs60t1Rj2V"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# STEP 1: Download large text files\n",
    "# -----------------------------------------\n",
    "def download_files(urls: List[str], save_dir: str = \"data\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url).split(\"?\")[0] or \"file.txt\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        print(f\"Downloading {filename} ...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        downloaded_files.append(filepath)\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# STEP 2: Read files into memory\n",
    "# -----------------------------------------\n",
    "def load_text_files(folder_path: str):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "            docs.append({\"id\": filename, \"text\": text})\n",
    "    return docs\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# STEP 3: Token-based chunking\n",
    "# -----------------------------------------\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    max_tokens=500,\n",
    "    overlap_tokens=100,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ") -> Generator[Dict[str, str], None, None]:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "    for d in docs:\n",
    "        tokens = enc.encode(d[\"text\"])\n",
    "        i = 0\n",
    "        chunk_id = 0\n",
    "        while i < len(tokens):\n",
    "            chunk_tokens = tokens[i:i + max_tokens]\n",
    "            text_chunk = enc.decode(chunk_tokens)\n",
    "            yield {\n",
    "                \"doc_id\": d[\"id\"],\n",
    "                \"chunk_id\": f\"{d['id']}_chunk_{chunk_id}\",\n",
    "                \"text\": text_chunk\n",
    "            }\n",
    "            i += max_tokens - overlap_tokens\n",
    "            chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYQ-FzXWT9G-"
   },
   "source": [
    "Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEYydvvTT7ne",
    "outputId": "43fd85c1-ce3e-47fa-d970-d6469718f755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pg1661.txt ...\n",
      "Downloading pg2701.txt ...\n",
      "Downloading pg1342.txt ...\n",
      "✅ Loaded 3 documents\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\",  # Sherlock Holmes\n",
    "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",  # Moby Dick\n",
    "    \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"   # Pride and Prejudice\n",
    "]\n",
    "\n",
    "save_dir = \"data\"\n",
    "download_files(urls, save_dir)\n",
    "\n",
    "docs = load_text_files(save_dir)\n",
    "print(f\"✅ Loaded {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBU9BB65WoXc",
    "outputId": "a761be12-7626-4e76-db24-93e04a352b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 1967 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks = list(chunk_documents(docs, max_tokens=400, overlap_tokens=80))\n",
    "print(f\"✅ Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBivmdn3UIm9",
    "outputId": "d2e0d757-a933-4d19-c38c-3d3a36354a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "doc: pg2701.txt\n",
      "chunk: pg2701.txt_chunk_0\n",
      "preview: ﻿The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts o\n",
      "\n",
      "---\n",
      "doc: pg2701.txt\n",
      "chunk: pg2701.txt_chunk_1\n",
      "preview: CHAPTER 11. Nightgown.\n",
      "\n",
      "CHAPTER 12. Biographical.\n",
      "\n",
      "CHAPTER 13. Wheelbarrow.\n",
      "\n",
      "CHAPTER 14. Nantucket.\n",
      "\n",
      "CHAPTER 15. Chowder.\n",
      "\n",
      "CHAPTER 16. The Ship.\n",
      "\n",
      "CHAP\n",
      "\n",
      "---\n",
      "doc: pg2701.txt\n",
      "chunk: pg2701.txt_chunk_2\n",
      "preview: . Ahab’s Boat and Crew. Fedallah.\n",
      "\n",
      "CHAPTER 51. The Spirit-Spout.\n",
      "\n",
      "CHAPTER 52. The Albatross.\n",
      "\n",
      "CHAPTER 53. The Gam.\n",
      "\n",
      "CHAPTER 54. The Town-Ho’s Story.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Show a sample\n",
    "for c in chunks[:3]:\n",
    "    print(\"\\n---\")\n",
    "    print(\"doc:\", c[\"doc_id\"])\n",
    "    print(\"chunk:\", c[\"chunk_id\"])\n",
    "    print(\"preview:\", c[\"text\"][:150])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
