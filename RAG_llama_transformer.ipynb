{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶ô Fully Open-Source RAG in Google Colab\n"
      ],
      "metadata": {
        "id": "LNrXrrX7jnNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q llama-index-core llama-index-embeddings-huggingface \\\n",
        "                 llama-index-vector-stores-faiss transformers accelerate \\\n",
        "                 torch sentencepiece bitsandbytes"
      ],
      "metadata": {
        "id": "_KT3cxAijaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1Ô∏è‚É£ Imports\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "7A9bXXMmjljV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Load your local data (put some .txt files in /content/data)\n",
        "data_path = \"/content/data\"\n",
        "documents = SimpleDirectoryReader(data_path).load_data()"
      ],
      "metadata": {
        "id": "pcWw6Mlkj4_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ Local embedding model (Hugging Face)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "YV_DIOjEj8IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4Ô∏è‚É£ Local FAISS vector store\n",
        "faiss_store = FaissVectorStore.from_params(dim=384)\n",
        "storage_context = StorageContext.from_defaults(vector_store=faiss_store)"
      ],
      "metadata": {
        "id": "akze0Ay1j95l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Build the vector index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "V-ouKJT7kBs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ws89OYijUxd"
      },
      "outputs": [],
      "source": [
        "# 6Ô∏è‚É£ Load an open-source LLM (via transformers)\n",
        "# Recommended: small instruct model to fit in Colab GPU\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True  # use less VRAM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7Ô∏è‚É£ Query + Generate function (RAG)\n",
        "def generate_response(query: str):\n",
        "    # Retrieve top-K context chunks\n",
        "    retriever = index.as_retriever(similarity_top_k=3)\n",
        "    retrieved_docs = retriever.retrieve(query)\n",
        "    context_text = \"\\n\\n\".join([d.get_text() for d in retrieved_docs])\n",
        "\n",
        "    # Build final prompt for LLM\n",
        "    prompt = (\n",
        "        f\"Context:\\n{context_text}\\n\\n\"\n",
        "        f\"Question: {query}\\n\\n\"\n",
        "        f\"Answer concisely using the context above.\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "yP_eAwGsjyul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£ Example usage\n",
        "query = \"How do I reset my device?\"\n",
        "print(generate_response(query))"
      ],
      "metadata": {
        "id": "5XuHqc__j0Ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}