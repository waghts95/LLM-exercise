{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFUk9INMXQct"
   },
   "source": [
    "This version performs adapter-based “continued pretraining” (unsupervised QLoRA fine-tuning) — it will run even on small GPUs like a T4 (Colab/Kaggle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8pAX2hKXJi2"
   },
   "source": [
    "1️⃣ Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ow9iMXLxWlz1",
    "outputId": "a9caa269-6fa7-4bb2-82ac-d928da424584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages (quiet install)\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q transformers datasets bitsandbytes accelerate peft huggingface_hub sentencepiece\n",
    "# NOTE: transformers pinned for stability with bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAZZIIhyXXuw"
   },
   "source": [
    " 2️⃣ Imports and Hugging Face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fo5TSXcee6ML"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login, whoami\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxSkZvRPfEGw",
    "outputId": "e09ac894-c27d-455a-abf4-2fde48e5f424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in as: twagh\n"
     ]
    }
   ],
   "source": [
    "# Use your Kaggle/Colab secret or paste manually (avoid exposing publicly!)\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Logged in as:\", whoami().get(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P9EwYMshXMqN"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85I-TVNEXkOt"
   },
   "source": [
    "3️⃣ Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ux-EAGIaXkzt",
    "outputId": "a38d7bbf-8108-4273-908a-dc940e52b129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded for model: facebook/opt-1.3b\n"
     ]
    }
   ],
   "source": [
    "# ---------------- USER CONFIG ----------------\n",
    "model_name = \"facebook/opt-1.3b\"       # <- Use an open model (fits on small GPUs)\n",
    "dataset_path = \"combined_dataset.json\" # <- Your uploaded dataset\n",
    "output_dir = \"./q_lora_continued_pretraining\"\n",
    "\n",
    "block_size = 1024          # tokens per chunk\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "num_train_epochs = 1\n",
    "learning_rate = 2e-4\n",
    "save_steps = 500\n",
    "logging_steps = 50\n",
    "\n",
    "print(\"✅ Config loaded for model:\", model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUY1SbTXqD7"
   },
   "source": [
    " 4️⃣ Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbuD81vFoya8",
    "outputId": "af9f8c53-3ded-46f4-d4eb-b4ce502654be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://tusharwagh.appspot.com/data/combined_dataset.json...\n",
      "- [1/1 files][  4.6 MiB/  4.6 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/4.6 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! mkdir -p data && gsutil -m cp -r gs://tusharwagh.appspot.com/data/combined_dataset.json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206,
     "referenced_widgets": [
      "c5a9f9e786714058a35f27a88a9a48e1",
      "665faf80aae2482fbead09197acaa646",
      "be7b7ee4f3564ff5807969baec194ecb",
      "1ccc189f4f5b455abf0a622d066d0939",
      "70897d461fcd4c68be6ba5908ab78a2a",
      "c5c843ffc1a240ee9ada1f5d48dbe133",
      "cd3133ce6155435495602b30950da08c",
      "f667954292cc48c0bac3deafd8b84e25",
      "35ffa819b33848f9aad32bf0c7ad791e",
      "711f62f69b0b4ee29ccc3903fa8d9abc",
      "35b0a23d36b841f79e3406e7c366b390",
      "11646b87e52345fab4ff24e4da61da28",
      "dcb5bd7547b94f0fbc2daa2841428af2",
      "d37d9f5378d4405999eea5c8cf0d5740",
      "a742d0a67b224a3ba334333e0c5698a8",
      "b7a78385ba1e4db5bd84dba27695150e",
      "ed9a6751641d45d9b2c0281c276ecf72",
      "b9bcbef007f949e9be17e34d1c9cd86c",
      "c5570485dfa843c596dc2a6336feab73",
      "173f1383a64d4b2995dee04f06ffd9dd",
      "e5e36bd0ae334154ad8b84fb7ccc1cf5",
      "2b8e6343dbce49a4820a15a07dd5177c"
     ]
    },
    "id": "zjg0sMfkXwyM",
    "outputId": "84616a3d-7716-414d-9b4d-ecb43493a6d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a9f9e786714058a35f27a88a9a48e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11646b87e52345fab4ff24e4da61da28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3512\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load labeled dataset (Context + Response) and flatten into plain text\n",
    "dataset_path = \"/content/data/combined_dataset.json\"\n",
    "ds = load_dataset(\"json\", data_files=dataset_path)\n",
    "print(\"Dataset splits:\", ds.keys())\n",
    "\n",
    "def merge(example):\n",
    "    ctx = example.get(\"Context\") or example.get(\"context\") or \"\"\n",
    "    resp = example.get(\"Response\") or example.get(\"response\") or \"\"\n",
    "    text = (ctx.strip() + \"\\n\" + resp.strip()).strip()\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = ds.map(merge, remove_columns=ds[list(ds.keys())[0]].column_names)\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgvfZW4gXzBr"
   },
   "source": [
    "5️⃣ Load model + tokenizer (4-bit quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309,
     "referenced_widgets": [
      "c7f95fd258a0454dbbff7cfd9003705e",
      "8de7fd9b4c77428db4312d6af01b4332",
      "353863478dfa470186f02148b6bc36b8",
      "04257157bcc1406d95acd6b1ba05a592",
      "943b2006684b4102b7bb51d4bc7f3835",
      "58f0df50b130472eba8124b1572fbabd",
      "b3658ef652ca4dd68053a5f13f611eb8",
      "f554bc42c34849e280042ad220af407d",
      "0c76d33c707743a68bd2246cc6313a2a",
      "3b20ecce48894123a1fb00cd7adabb07",
      "4e72cc947b2b415e86bb5b3bded52dbf",
      "e6cb0ff3462a476a990257d3036ce9fb",
      "9cbc96a3819849d388c3ba45b9c6c18a",
      "3d33af2d662b4b21be513599a7cd1ef0",
      "026f76a9a15c4b609382c7e85f2b2f0f",
      "d58a86a9349d4879bc3ce86544e0c042",
      "0e8b72a295c64312817acba572c36803",
      "e8b0fc7376994bf09902cf29e4eccf7d",
      "6dea621f4e9849edaca0b8a8edf5a833",
      "8aae7f35798f48418017309d7778879a",
      "c0fb1a972e5347249126bf3c654594c6",
      "132be32fbd0f49129f058dddfbaf9419",
      "c7e455258b0143be84669e96f5f071dd",
      "9b377a05c39048e68ab6be6a95dc29be",
      "72689776f83c420eabfe8e45145353ae",
      "bd41bf6fa3c946a1a5f33b642606d2b3",
      "92b056691d1d41438a8f0d03f9dbcda4",
      "acfb9edfd402410fa00dfc7c00da0dd3",
      "c03b8bd8818f4fa7b999ff1c23f20b3d",
      "a4642271b63d4923a303eb890ca58947",
      "847e7600197f47e2bfc835fff22b9833",
      "4a083aa82a9840f09b53c973092ed7eb",
      "83110aa6dcb240dbb74db23d97dbd9d9",
      "113c0eb6dd904c81bc8543940a6a7b3d",
      "4c5273c044d14b6eb1538628a1a4776a",
      "de8aa118b4664df29025dca5716d734a",
      "0418bf27ed6041ffada07b69ba285115",
      "e55c3c1eb19e41568b860ed749d5de57",
      "bb71e5ef09a743de9c2357e1c18fc5c9",
      "0ac5e28346204c94a0bb9dd135c53416",
      "8d331cf8e4dd4a8a8f13514f8513f290",
      "ed9f734e457440e8ad169e135fe7c5bf",
      "7aa4d9b4b10f49fa83e866482b2046e5",
      "1de86de8da134acf86883036b2bd9c31",
      "b5bb2a6fd0114336ba3f1ec600e364fb",
      "404d8ac29c6a46efb5f4582e3e694972",
      "0501b5a2d61d4263bb9ff0c347b665d7",
      "f8e3f5c3d4f948598bd80e6e63d8cccc",
      "9066d1c84bc94edabcb72eca7455d62b",
      "ae612794fdb14f71b0270c104387e36f",
      "499d48815fe54338bf0d5603401a500a",
      "ea1e05078063474e8ca97c7445f0dcca",
      "9602d28317c5439eb8a6e6fa9d16c3e8",
      "4e5b1d7dc42849f98f7de05405cae245",
      "6014fa218ed24e7690284332fced4862",
      "20edaa4f90bd4a27acbd3d0d2a069a83",
      "640a65ce9b424395968ce8a251d19e25",
      "3bfce31512284ef9ba7257474550cc0b",
      "5c835f5752c244d4bf1b2b669d088379",
      "50d83d70b68d4616aef7f74a78ec000e",
      "0f85eb9a728049c9a758ebc592943cae",
      "7e9f6e03951944fbbcdaa5aa9c2f622d",
      "03599e1636fb4ae5935e7286cb89d791",
      "ffef0d291eb54bc8bc158c5ac2b3504d",
      "79a071d7a12d49cba1a59a1e6443887d",
      "db56ce00d3f1467ba7b3f3d8acd1efb0",
      "8789209be255426baf1df72c9d5be829",
      "688cb6ca4a594f188ea84317f5eecce0",
      "e8c1f3de524e435a9c23682022def61c",
      "91c7ca972f024d689cc12de7214d1d11",
      "ef82bfcc68ae43228489ae3f2d989c2b",
      "3ea118ad5f964f0b9910221da2c5f7eb",
      "6add56a187c64627855d828527530071",
      "db8b9049269d4ceca2c279a7dd872ab4",
      "2dc0434810374b37bd8b26b14f462706",
      "c91f09d84a9a4e19b21920142977dcc1",
      "366f9ec84fac486286857d7eaa61262a",
      "88d3760de89f4bfba151b069bce7ba8a",
      "1a5aab8a039d44a58c6c569c2279aa0a",
      "467a7d5e479744008d0d95fcf991cfbb",
      "b11c7fa1df084a1c85004847348dbf3a",
      "3c73f9f324c9485489a72de0906cb4ba",
      "723f193fe7084ef183b7e5dc9450246e",
      "add9fad0039e477a9fe795f56fefa074",
      "f9bfbb197585495fb09fd7ec446abadc",
      "0e5e10c1a4a04a149b5050b71caafd2e",
      "69c7ecbb304d40548c856cf1ebabcdb6",
      "05111d3c2b9142548ef266ec4137b344"
     ]
    },
    "id": "SBxITHt1X1QX",
    "outputId": "e808b489-78fb-4228-9931-06d9b7130271"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f95fd258a0454dbbff7cfd9003705e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb0ff3462a476a990257d3036ce9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e455258b0143be84669e96f5f071dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113c0eb6dd904c81bc8543940a6a7b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bb2a6fd0114336ba3f1ec600e364fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading model in 4-bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20edaa4f90bd4a27acbd3d0d2a069a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8789209be255426baf1df72c9d5be829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d3760de89f4bfba151b069bce7ba8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model prepared for k-bit training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "print(\"⏳ Loading model in 4-bit...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"✅ Model prepared for k-bit training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUtjsQ6eX5tW"
   },
   "source": [
    " 6️⃣ Add LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOa4n2EaX7Jx",
    "outputId": "779cd690-1c19-4a37-8c11-00a41470218a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA adapters added: 3,145,728 / 714,924,032 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✅ LoRA adapters added: {trainable:,} / {total:,} trainable parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4ngLqtLY4Px"
   },
   "source": [
    "7️⃣ Tokenize + create LM chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "e193a7b0376d429a888014155200d0ac",
      "7484a70c912e43e780a947619948578d",
      "0408b30ded474e4aa109e467700d3ffe",
      "ff5538222cdc437389732c65eb568f6d",
      "3846ef687aeb4008995a62b3d987c68a",
      "2d83e1913c4b403499cb0e1bfc694bbe",
      "70c40758c63f44f48cea7cf2d4086e1c",
      "1b8c7301c6c5454aa5e305fc25252b10",
      "5e1c2fa832ff47948f0b8eb6062c17bf",
      "4b5dfb85c9ff425aa7ff2c724ddebb0e",
      "d9dbe95a6cf34828a174e4601884bd44",
      "ee8493a7efc043e4a3c66dcc93259012",
      "a9a9de9ad68647b1aa7995f12504f747",
      "704d573a5732405a921e172c88528198",
      "96bba3c38d5c4247b46d67bd675ba257",
      "2d4433a6359d407790b29612250f8845",
      "6dbcd3aac9d347328ad297c70070204b",
      "e59aa8410f4b406dba1b86a0c8e3b1e7",
      "2bfa6edc7a7e46fd858176fdc7eb79cf",
      "252ac948bb434b5f8b562c4df487a294",
      "a42a33d3104c4eefbfd760f04428cf04",
      "ae0ee7bb13a0419890bd460a47f60d22"
     ]
    },
    "id": "VJeGnjumY_tF",
    "outputId": "7e9e19d2-f1a7-46e4-f06a-9b80a4705a5f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e193a7b0376d429a888014155200d0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8493a7efc043e4a3c66dcc93259012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LM dataset ready: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1028\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False)\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(v, []) for k, v in examples.items()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = [ids.copy() for ids in result[\"input_ids\"]]\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)\n",
    "print(\"✅ LM dataset ready:\", lm_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUt7nYacZbFN"
   },
   "source": [
    "8️⃣ Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2atqyf7ZYup",
    "outputId": "8535ffee-cd43-4452-d999-f6efc5dbf71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer initialized — ready to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3583927193.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[list(lm_dataset.keys())[0]],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized — ready to train!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BaaL5wxZeCt"
   },
   "source": [
    "9️⃣ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "nSIqgbcBZgc8",
    "outputId": "c5103a32-38e6-4c55-ebed-99d5d01767b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 09:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.631200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=2.611324721116286, metrics={'train_runtime': 599.2037, 'train_samples_per_second': 1.716, 'train_steps_per_second': 0.108, 'total_flos': 7653441367179264.0, 'train_loss': 2.611324721116286, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3O87tgdZlJi"
   },
   "source": [
    "🔟 Save adapters + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zG-n6JVotXJa"
   },
   "outputs": [],
   "source": [
    "! mkdir -p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4x2NRtBZisB",
    "outputId": "09339e1e-1b19-4ba5-cc34-448b5c3366b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Adapters + tokenizer saved to: model\n"
     ]
    }
   ],
   "source": [
    "output_dir= \"model\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"✅ Adapters + tokenizer saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GxHkmE7ExdEj"
   },
   "outputs": [],
   "source": [
    "rm -rf q_lora_continued_pretraining/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDC5iYwxyPUs",
    "outputId": "007fb475-387f-4f55-8b47-48026a80b9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model/ (stored 0%)\n",
      "  adding: model/vocab.json (deflated 68%)\n",
      "  adding: model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: model/merges.txt (deflated 53%)\n",
      "  adding: model/README.md (deflated 65%)\n",
      "  adding: model/tokenizer_config.json (deflated 62%)\n",
      "  adding: model/special_tokens_map.json (deflated 79%)\n",
      "  adding: model/adapter_config.json (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!zip -r model.zip model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
