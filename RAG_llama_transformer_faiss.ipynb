{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNrXrrX7jnNm"
   },
   "source": [
    "# 🦙 Fully Open-Source RAG in Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b841618e",
    "outputId": "160bb40f-2fd1-43ff-d706-3beae992a6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.8.0+cu126\n",
      "Uninstalling torch-2.8.0+cu126:\n",
      "  Successfully uninstalled torch-2.8.0+cu126\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m545.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m918.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.8.0+cu126\n",
      "    Uninstalling torchaudio-2.8.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# Uninstall the current PyTorch installation\n",
    "!pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 compatibility\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KT3cxAijaWA",
    "outputId": "531c8259-a1aa-4567-e3b3-4f89c41c4e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q llama-index-core llama-index-embeddings-huggingface \\\n",
    "                 llama-index-vector-stores-faiss transformers accelerate \\\n",
    "                 torch sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "fasww4QKJjSO",
    "outputId": "4da1cb63-dd2b-4372-baa4-10732fbe7154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu-cu12[fix-cuda]\n",
      "  Downloading faiss_gpu_cu12-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting numpy<2 (from faiss-gpu-cu12[fix-cuda])\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12[fix-cuda]) (25.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12[fix-cuda]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12[fix-cuda]) (12.1.3.1)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_gpu_cu12-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, faiss-gpu-cu12\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-gpu-cu12-1.12.0 numpy-1.26.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "0d6f664931c842e5929e3d4b769321a4",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install faiss-gpu-cu12[fix-cuda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16lvpU3kJWN_",
    "outputId": "f5894f80-520d-4e7b-b82f-54e19644f3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# testing number of GPU's assign\n",
    "import faiss\n",
    "print(faiss.get_num_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A9bXXMmjljV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1️⃣ Imports\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcWw6Mlkj4_O",
    "outputId": "1cb2601a-3d40-4332-a90c-d27c8d1aef47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.core.readers.file.base:`llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n"
     ]
    }
   ],
   "source": [
    "# 2️⃣ Load your local data (put some .txt files in /content/data)\n",
    "data_path = \"/content/data\"\n",
    "documents = SimpleDirectoryReader(data_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcxYirGIb2Y1",
    "outputId": "0a501541-8282-44df-c525-79c7f5e1691f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='3585ff15-7d76-4b41-869c-6efeecdc3d08', embedding=None, metadata={'file_path': '/content/data/Resume-Tushar-Wagh-8-yrs-ML-Data-Research-txt.txt', 'file_name': 'Resume-Tushar-Wagh-8-yrs-ML-Data-Research-txt.txt', 'file_type': 'text/plain', 'file_size': 3772, 'creation_date': '2025-10-07', 'last_modified_date': '2025-10-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contact\\r\\nIndia\\r\\n9890132816 (Mobile)\\r\\nwaghts95@gmail.com\\r\\nwww.linkedin.com/in/tushar-wagh-1781b7112 (LinkedIn)\\r\\ngithub.com/waghts95 (Other)\\r\\nTop Skills\\r\\nPython\\r\\nData Science\\r\\nCloud Management\\r\\nTushar Wagh\\r\\nData & MLOps Engineer\\r\\nPune, Maharashtra, India\\r\\nSummary\\r\\nAs a leader,I believe that every brain is unique.I believe in 1. Fostering a culture where vulnerability is accepted.2. Growth does not mean to win everytime, important is you walkhand-in-hand.I firmly,Encourage and respect: Ideas, creativity, art and innovation.Believe in : \"Take your time and get back with something never seenbefore\"Technical skills : Data Science : Machine Learning and Deep Learning algorithms forComputer Vision and LLM.Data Engineering: Data Pipelines and Data ProfilingTools and Technologies:1. Python : Have hands on Deep learning, Machine Learningpackages, Image processing packages, Data Processing packages,Numerical calculation packages, Data Visualization packages, DataQuality packages.2. AWS : AWS Data pipeline, EMR, Sagemaker, SNS, Redshift3. Azure : Databricks, Azure Data Factory4. GCP : Have hands on Vertex ai, Google cloud functions, CloudRun, App Engine, Cloud Storage, Pub/Sub, BigQuery, ArtificialIntelligence, AutoML for Vision.5. Big Data : PySpark6. Web Frameworks : flask, streamlit, fastapi.7. Database : MySQL, MongoDB, ElasticSearch, PostgreSQL8. Orchestration : Airflow, rundeck9. containerization : Docker, kubernetes9. Monitoring : DataDog\\r\\nPage 1 of 3\\r\\n10. Other : Blockchain\\r\\nExperience\\r\\nFairtility\\r\\nData and MLOps Engineer\\r\\nMay 2025 - Present (5 months)\\r\\nPune, Maharashtra, India\\r\\nAs a Data and MLOps Engineer at Fairtility, I design and maintain scalabledata pipelines and infrastructure to support AI-driven fertility solutions. I work atthe intersection of data engineering and machine learning operations, ensuringseamless deployment, monitoring, and automation of ML models in production.My responsibilities include data preprocessing, model versioning, CI/CD for MLworkflows, and enabling reproducibility, reliability, and scalability of AI systemsused in IVF clinics.\\r\\nOptum\\r\\nData Engineer\\r\\nOctober 2023 - April 2025 (1 year 7 months)\\r\\nPune, Maharashtra, India\\r\\n• Designed and implemented data pipelines for processing large-scalehealthcare datasets, ensuring data integrity and compliance.• Leveraged AI and machine learning models to derive actionable insights,improving patient outcomes.• Utilized big data technologies like Hadoop and Spark for real-time and batchprocessing.\\r\\nInnova Solutions\\r\\nData Scientist\\r\\nMarch 2022 - September 2023 (1 year 7 months)\\r\\nPune, Maharashtra, India\\r\\nI designed and developed AI-driven models to extract actionable insights fromcomplex healthcare datasets, improving patient outcomes and operationalefficiency. By applying machine learning and deep learning techniques, Ipredicted disease trends, optimized treatment plans, and enhanced clinicaldecision-making. Collaborating with cross-functional teams, including cliniciansand business stakeholders, I translated data-driven insights into practicalhealthcare solutions.\\r\\nBioEnable Technologies Pvt Ltd\\r\\nPage 2 of 3\\r\\nData Scientist\\r\\nAugust 2019 - March 2022 (2 years 8 months)\\r\\nPune, Maharashtra, India\\r\\n• Designed and trained machine learning models for object detection, objectsegmentation and facial recognition.• Utilized tools like OpenCV, TensorFlow, and PyTorch to optimize algorithmsfor performance.\\r\\nzCon Solutions\\r\\nData Engineer | Data Scientist\\r\\nOctober 2018 - July 2019 (10 months)\\r\\nPune Area, India\\r\\nEmergys\\r\\nData Engineer | Data Scientist\\r\\nAugust 2017 - July 2018 (1 year)\\r\\nPune,India\\r\\nEducation\\r\\nPune Vidhyarthi Griha\\'s College of Engineering and TechnologyPune\\r\\nBachelor of Engineering (B.E.), Computer Engineering · (2013 - 2017)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473,
     "referenced_widgets": [
      "d27c51ded351468ea4ab332246eef60a",
      "e5d6d045d25448e2bd0fadfa788a2396",
      "404ac11a24d3450b820e31b64104c783",
      "a71e5f24e70e413a8b8786d192fa3c02",
      "8da4fd1225534490af7cff5cded7c101",
      "718bf8da3f664fd0880430611c203ae0",
      "7752af25f6b14282864b57752a9b2134",
      "9910990f45c14a6ca901461be1aa2963",
      "ffe1fa52322c48688a88e6fed4071860",
      "218d1b46425f47c8814ff8350b8a440b",
      "959faace70f643c882fc6398a80212ea",
      "f1b1bae88e724d85b8893992720c9e1c",
      "171c5f0d069045af805ea5b98cd0774a",
      "9f0acaff1666471bbaf810a3d0ebff7e",
      "ddb3265865b2486eb1b0cd64ab4e8bb9",
      "a904faa645644d7eb4623760b27bb415",
      "b880a32eee904305b6b608917890c5e5",
      "c3b66b7c658449bdbc9e864db78f15f6",
      "19f0bfa5f5a54c5caa9861d62fffa0c8",
      "c22619d738f1449596f2f8ed64d15eb2",
      "eab5d83c8445405bbc84adc1f957b35f",
      "dfc8ef46142f473d81fa19c7162860a0",
      "f8bc202e1b674356887f3572b51e6fbe",
      "2b53ed93e7404b79a3c82746a5c4085f",
      "101bedd7d6db42dda454529ce09d1832",
      "973294cde3684f2c9d4c2cc06a72e556",
      "ff24776914f247ecbd4f9625af5bd7f5",
      "3b67cb0cdd954e028700a1b0669faee9",
      "93ff2e13216546d0b03103d6dcc6fceb",
      "cee353a3832e45e181faaf941bd4a245",
      "84a477d0bacd44e7b8e1eec03951dc15",
      "0918ce1a00e6492a9e438f3a4b9e10c8",
      "84c34ada0e554830b7058b33ef05d98b",
      "eb6395b790c74281abf8c602fecdc2de",
      "8d49a0a8706e4993b26fddd9ecdffa46",
      "6793fe0f74da4bd489ab0cd693689940",
      "5696ac57b58a4fe9b068df9a4a6927f8",
      "caf8486755304e0b90070307e19f5aa4",
      "09cfeb9bb0e74ffd8ae5e20a5552d936",
      "d9567af84f9548c69321a45b9b3cf39a",
      "a60cbe376f984ee288f35e895bafb9f4",
      "5a4b9378e33647be889f056eaf9ba364",
      "7cc98b0a71ee41d7ac915a7da7f329ad",
      "c1344cc5e36349f0957f28e33a8bf74c",
      "97071c81d9f64c3b9ce1644b87156fb0",
      "387d8244419a4b3aa6ab502b82406b93",
      "a251526fa61644d89d3e8177557bdef0",
      "ba88c6a97768446a90cf3a291b3a220c",
      "1e31d25eda554f51b1f1427416558b3f",
      "a6a1d50405524e1bad881883c1abcca5",
      "855c8853ae8c43d1b366372e62bb5807",
      "e8fa143c14cb4b0d93174b116e7dee47",
      "f9e7add220884c8986d10b356f992944",
      "71e494d5b7754081b95814e97568d45a",
      "dc12a23b8f3a4167be065cef45c4b4f9",
      "df35f0563df94646b5eb86527955efc4",
      "653e306eef2743629917784495477cdc",
      "b6b481641f444c6693d51af5e9d2b8c9",
      "9184880f0fb24bb3830dbe70225bd4b8",
      "44e841a8d37d4275a5d568ece94ddc10",
      "67aeaac98f2a44dc92f8984ed5667ed3",
      "120c502042874c1ba8fa5f5e8e489458",
      "ee9ac6f920cb44ad830377d55a941670",
      "220a08ccc9504526a54226b0dae7572a",
      "8b2525071cc54ad49b23bae65143c7c4",
      "6278c1ab1e434109851ca8a10b0d7dc2",
      "b825616cacf54e67a3cd1b59aac5a13d",
      "b75a2d85b3e74effa65394349af78738",
      "5cbaca0e022f41e6a99ea1490b043551",
      "2999f663d2114173ba0fe5375697712e",
      "2a24a2a05c704a79b7660679286f1f74",
      "d4218fd568714674a2a2bd54143d89e9",
      "f743d6ffac99440bae20250a60a6ae31",
      "dc6bf15016ca44858e8162e828bb95a3",
      "d0ebf99421c349fa9dc4395e7b8a347d",
      "20c173cfd324413aba924716a4c9b999",
      "d214662e496e43229225501beeac3827",
      "f890ec3602444879a9d9fac7f89539f2",
      "0c897b0fb4104bdfa055393afc0e4453",
      "d930acea6e984bc28754d655a132088e",
      "c337374c07bc444493ec11445453e719",
      "29fb10eec8314e3b961266eb9c6f0ace",
      "d5fc438cdfb3437b994fc7b0bbbc7196",
      "8f335f4bd1f84a589248af042de175e3",
      "773ba007faf0409c8778862de9346fd0",
      "e6a2cd93b41e4314bff1a9f2dccf1b4e",
      "3e913c38f9ad4c97ad82181074b98b4e",
      "6225b3b72fee42aea49aa3e7189982ed",
      "84a767e537a7434c8de8aa0596eb05b1",
      "a80aaed383324a64ab29ce11b0620999",
      "7d81bcf92b754ec4b1e4b2cc05ec387d",
      "2f04a2af4d6a4cd0a50125ff58141514",
      "54f1b92c99b746499b62004bafc1096e",
      "0d21f9f7b3b64aa48b2e09bcfc57c768",
      "54c7f61db9b34429a206117bca3fdfcf",
      "8f370b56565b48d9a0bdbb7a0e9f917b",
      "13048da6dfa54d6a840ed2fbe50ae2ab",
      "ed7b0aef07dc4d589e43c3393e751ba6",
      "9d6dec54e7e344288393951189a0f70a",
      "724f25f82ebd45d484dd6414004b4109",
      "09b6aa92938947c3a2a70101ef1ebc94",
      "dc9b64be2aee45d18ab764b0590ffb5d",
      "69bf0d7b536648418568997bc43a9ee1",
      "9a07480fcda341eebaa6d9eea227f025",
      "9f70f2f0834c4f4a888bad57f7c4ac5b",
      "9a3c6a268f364552b9bf619a1cc289f8",
      "f0d7783c6c6f4efca9c6c6d862b8be67",
      "de69622f8ef440019cb6f77cb4707969",
      "8576b937c0b44a65aa8ff7f2eb481755",
      "24d851672d3642a4b59419c3a8a8b6e8",
      "772c70c9136945918d2a29130a247947",
      "5d582be6d8704c42b77ae4c040e03eb3",
      "5caa7eb1d03b4e4da3afc91990705f45",
      "5d5e424d1ba9478ba66c0a91925c5dcc",
      "850cd10140bb46a7ac5c8bd8b1d57882",
      "e5d6d1869a5347e79aeb30593a6127a2",
      "a0bad7e5264c42bc8616c0d5da819b62",
      "cc5921a73d384109ab466122f387b23e",
      "30f3d7648049454998ddd02e2c0ee278",
      "adbb4c33192143bb9782720017788636",
      "4aeccdf584ac4904ac52e78df38879b6"
     ]
    },
    "id": "YV_DIOjEj8IV",
    "outputId": "cd8215ca-825f-4af8-87ca-a77e63607dcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27c51ded351468ea4ab332246eef60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b1bae88e724d85b8893992720c9e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bc202e1b674356887f3572b51e6fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6395b790c74281abf8c602fecdc2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97071c81d9f64c3b9ce1644b87156fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df35f0563df94646b5eb86527955efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825616cacf54e67a3cd1b59aac5a13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f890ec3602444879a9d9fac7f89539f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a767e537a7434c8de8aa0596eb05b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724f25f82ebd45d484dd6414004b4109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772c70c9136945918d2a29130a247947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3️⃣ Local embedding model (Hugging Face)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akze0Ay1j95l"
   },
   "outputs": [],
   "source": [
    "# 4️⃣ Local FAISS vector store\n",
    "import faiss\n",
    "\n",
    "# Corrected initialization: Create a FAISS index first\n",
    "dimension = 384 # This should match the dimension of your embeddings\n",
    "faiss_index = faiss.IndexFlatL2(dimension) # Example: using L2 distance\n",
    "\n",
    "faiss_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=faiss_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-ouKJT7kBs9"
   },
   "outputs": [],
   "source": [
    "# 5️⃣ Build the vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7083939"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# @title Login to Hugging Face\n",
    "# @markdown You need to accept the terms and conditions of the model on the Hugging Face website first.\n",
    "# @markdown You can get a token from your settings page: https://huggingface.co/settings/tokens\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "  HF_TOKEN = input(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436,
     "referenced_widgets": [
      "39495f8f44574b6190d166510c48ce8d",
      "ac8025e387a741d2a7a6190bceb8ce73",
      "b4e59a28f0b44ac2ba030344bf4d5e87",
      "d212bdccb5144717a706ccbb7d574ec6",
      "f4e2bb0e1756408aae187953df784de6",
      "ada0c1c32f1f4ffe84ab38120b656e84",
      "5cedb0ffb40646e58d72dcda0f43d552",
      "89d2a0a41a544472bc540d2681d3b729",
      "b6f2292470e0445ba271cc5654fc6e48",
      "2aaf98d4e81e4327aa967470c9fddb15",
      "40ab96329609412bb990ff11bd08bc22",
      "9d63fb01adcc4e72a22a27007dd5d338",
      "ebf2206100914ef7a0919f5fe76e0821",
      "bab485dcef9346a3a7e16e67c9d5bf2e",
      "2f54513bd5a348eba3ac60393c506bfe",
      "22f2c7e79b5d46ff9049e5e99dfb185a",
      "5f5deb1510534901a9f33638aa92b29c",
      "a92efc68035c4189a48c40ac2b5af3fa",
      "40f5d5644be64a3e9a8c0274501c37a3",
      "b3d5d99a14604a4db653745db3bc0024",
      "7ef3b589933f4ef38e3b5ae139764c29",
      "19271c3f29f0413f9301027a6281e742",
      "594a93503f5746a0bef0bfd38d0570d9",
      "43ef0bab175a46c6a2836605fb50563f",
      "06f0bb0ad60c468280c40b8b153950eb",
      "f8a4ef5985ef424084a8a66caf8c34b5",
      "7f2125f487314f5a8dd63112b0d61626",
      "b72e681713a542d1944b330f6993872b",
      "d3857cff8806419cafb43f1fb1679be8",
      "16687169be524ab8b67557c5b26e3abb",
      "64321e0977e14ea198069ed870919ad0",
      "5cf02d90e9e945c58b149ca79ae55ad2",
      "85a39b2285f7494a80f74ae29ec49c4f",
      "0de72ef6ca0b415a87f9a1fe26ebd8fc",
      "b0ab5f3ee62841cbbb613802c7f34bd5",
      "e2fdb60ab440411cb0f2d5be0df22385",
      "82b268b2d5374a409aa291f3df4231e2",
      "51e22c9a65744684a53d0221028abac6",
      "5f85dbcbfb3043b59a89b79b30a8e38a",
      "9dd2ee0a21b4421e83442a57a25cdf6b",
      "f782a74e538d4549b03cc929b70884e2",
      "0e932371969a44e4820802728e826228",
      "872255183836416481c24b86b6790809",
      "c770e88d95664be48eb799c449afd992",
      "ddf9323acc5844a4b5516ce25f6a102b",
      "a50c32dfe9814afd93d22dc9c7988e77",
      "7a4b5e30f5ce47f69797e0f1bdb306b4",
      "a03756810f1245f5b8c54dd63ccaa7a2",
      "538437af1f694a97ac7d2995cc579973",
      "25494b52b11c4698b372f68153f9d227",
      "fb6f60f8820948d1969341ba70c23b0a",
      "7e09b8f53ded47caa78262f9d8477847",
      "eed5b62bc13b40ee895e3503b546f92a",
      "4e2916c3c239499f92a68b3d4c1e81fe",
      "0e957fbd1d52476195b4fcf02520c087",
      "d0d9395330b74cd3be4122907d1982b8",
      "cc0a390ae1e74438b3d0fbee0d020249",
      "d50e5e20272f400aa10c48e18cdcf090",
      "7fedac6998b647cab6dba17cccca311e",
      "1c326f242c834c08b6cfe0cb51c0d9d2",
      "e4afc676049740578c37020b0697d39e",
      "98866c9e441a425c9d78f71166010381",
      "f03501f8288c4c4aaa9439375f34721e",
      "ba0a703e1d9040fcb7503e6445969992",
      "5951d2b3da3c49f09a912c6d0f7c4424",
      "f6d6bf06bb7046dba44ed9e5cbb801ce",
      "d5b0c519f2f9474b87bcd10c0446b713",
      "24935e30e6174270861c294dad057f51",
      "1214263736e64624a5b9dc8fb2b2bfc9",
      "4b8884f21c3b4179a56d502cb84ef15f",
      "bcff71fb20024c53a811960da3711586",
      "96d1af564f8c46a4bf0f641a01a917fe",
      "bf4ced36cf2b439aad97f7313e336bf2",
      "f854d799731147ce9d424e30225d273c",
      "b95c988de24c4155a6b895ac4d0d17e3",
      "37f5fd830f814a6c8cfd86df76238376",
      "75d0ac74553349c1ae5341bb5e7f3ef3",
      "cab94dfe1f4248b79ebd7aca306206fe",
      "dfdc6523f64f4974873c754109506c61",
      "58db3d08c6ae465cb24e53b69881806b",
      "5f59c6cdc52b4e0d88d09f9e56699306",
      "ab8f8c8e8df440209d4540a67651511c",
      "18591b14ab4c464da24abe141dc122a5",
      "129d60c19d354872bf3e166c4955f8f2",
      "314fa70f8f2040dc8a8837d78c2ad144",
      "e5f9a4b3aac94f5caa7c6d7a1073ff9b",
      "c7782e97fb27461dac80093389048e57",
      "3b65cefa74c4438b8720dc997583355a",
      "0a11fec89c2a42ee9336b689975009c6",
      "c477ccd9147b43408e111a43fb9b8761",
      "99b9a79ea2814491a2f1610940509db2",
      "03310cac29ab40bf9d15363627dd2dbd",
      "60cec36980164cab83239d816c6bb6d0",
      "d9836f4b66414697a0aed0ae5a2e6164",
      "f72045feee8b4bed84cba8d76e3a6393",
      "0b7df6500669467ea5618fa2d333176e",
      "efe76bd12c184d9d92e9825845c02795",
      "5af3bc6ca9fa4a2fac0b8f72218b0a57",
      "fd9ccf009a644bccbb9fac1218eef5e4",
      "6427fa922b944a56b22642f05fc3e903",
      "9a3e0c9ea29042649e0c674636c58001",
      "bfae554c83a94f80affaa6c956b0f1fa",
      "ff7205d2fa05448cb4fb9a04ccd3a38d",
      "fdbca42874d54370a678f096907239f2",
      "64ec6a4b52424b278254892af8197f71",
      "e61c76e2d6224052b5431db40c6fbd35",
      "5156bfd33c1048a78cc30a0e6a446555",
      "4b62f0a1b95e465d83b0661976d17b96",
      "69e5c2a43b244032b448ff37e76646e6",
      "6697b4d4ed5b4e859802f0aed3872c14",
      "bd77505a202940cc8b9f692f76bc246d",
      "e0094db612584202b0549db81c06dab6",
      "ed58509b46b54b34bf42e8b21bfa72df",
      "1d072ba7906c4f5caf435ec575b616ca",
      "b289a31d932547039898d65c04ded716",
      "d0364ff2079841c8bfb29dc0ca31767b",
      "f2224ec956c1477ca2c7715c33b562fa",
      "d874d7a26dfc401eb74fd702e0fb660f",
      "6dd1f39f10c942e19433950ed9d6a7f0",
      "a3dd3f1570864367ab5e393df57b3e06",
      "19d290f5cf314063809fe59ba0de1407",
      "5d5702ec05784fd48c84bea2b7df15ee",
      "1dd983060045451c8375cdab82c32e27",
      "b24011a2fdf145328cf8668377ef947e",
      "6ff4fabc861747e89ca64a38cb56fab1",
      "c5c44cee0ae1493fb213e84d22064897",
      "b819c2578f3a47d1b35e3f88247d86b7",
      "3349dfcdbea646738e52815054ca36b3",
      "9506f7e304814e368bb19f021847b7cc",
      "b2ade8a72dd844baacfd228b1e6e0729",
      "92809d9f61534b6cb9c3701f88f3abe0",
      "fbe9c31e646c4c8bbf7b9d93a8ec0ddf"
     ]
    },
    "id": "_Ws89OYijUxd",
    "outputId": "fe0f0af1-c73f-473b-e5bd-d9581317bbd9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39495f8f44574b6190d166510c48ce8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d63fb01adcc4e72a22a27007dd5d338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594a93503f5746a0bef0bfd38d0570d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de72ef6ca0b415a87f9a1fe26ebd8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf9323acc5844a4b5516ce25f6a102b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d9395330b74cd3be4122907d1982b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b0c519f2f9474b87bcd10c0446b713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab94dfe1f4248b79ebd7aca306206fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a11fec89c2a42ee9336b689975009c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6427fa922b944a56b22642f05fc3e903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd77505a202940cc8b9f692f76bc246d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5702ec05784fd48c84bea2b7df15ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6️⃣ Load an open-source LLM (via transformers)\n",
    "# Recommended: small instruct model to fit in Colab GPU\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True  # use less VRAM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yP_eAwGsjyul"
   },
   "outputs": [],
   "source": [
    "# 7️⃣ Query + Generate function (RAG)\n",
    "def generate_response(query: str):\n",
    "    # Retrieve top-K context chunks\n",
    "    retriever = index.as_retriever(similarity_top_k=3)\n",
    "    retrieved_docs = retriever.retrieve(query)\n",
    "    context_text = \"\\n\\n\".join([d.get_text() for d in retrieved_docs])\n",
    "\n",
    "    # Build final prompt for LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n{context_text}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer concisely using the context above.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XuHqc__j0Ld",
    "outputId": "7fc5dfa6-cb8f-4d23-c42a-136f60103064"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Contact\r\n",
      "India\r\n",
      "9890132816 (Mobile)\r\n",
      "waghts95@gmail.com\r\n",
      "www.linkedin.com/in/tushar-wagh-1781b7112 (LinkedIn)\r\n",
      "github.com/waghts95 (Other)\r\n",
      "Top Skills\r\n",
      "Python\r\n",
      "Data Science\r\n",
      "Cloud Management\r\n",
      "Tushar Wagh\r\n",
      "Data & MLOps Engineer\r\n",
      "Pune, Maharashtra, India\r\n",
      "Summary\r\n",
      "As a leader,I believe that every brain is unique.I believe in 1. Fostering a culture where vulnerability is accepted.2. Growth does not mean to win everytime, important is you walkhand-in-hand.I firmly,Encourage and respect: Ideas, creativity, art and innovation.Believe in : \"Take your time and get back with something never seenbefore\"Technical skills : Data Science : Machine Learning and Deep Learning algorithms forComputer Vision and LLM.Data Engineering: Data Pipelines and Data ProfilingTools and Technologies:1. Python : Have hands on Deep learning, Machine Learningpackages, Image processing packages, Data Processing packages,Numerical calculation packages, Data Visualization packages, DataQuality packages.2. AWS : AWS Data pipeline, EMR, Sagemaker, SNS, Redshift3. Azure : Databricks, Azure Data Factory4. GCP : Have hands on Vertex ai, Google cloud functions, CloudRun, App Engine, Cloud Storage, Pub/Sub, BigQuery, ArtificialIntelligence, AutoML for Vision.5. Big Data : PySpark6. Web Frameworks : flask, streamlit, fastapi.7. Database : MySQL, MongoDB, ElasticSearch, PostgreSQL8. Orchestration : Airflow, rundeck9. containerization : Docker, kubernetes9. Monitoring : DataDog\r\n",
      "Page 1 of 3\r\n",
      "10. Other : Blockchain\r\n",
      "Experience\r\n",
      "Fairtility\r\n",
      "Data and MLOps Engineer\r\n",
      "May 2025 - Present (5 months)\r\n",
      "Pune, Maharashtra, India\r\n",
      "As a Data and MLOps Engineer at Fairtility, I design and maintain scalabledata pipelines and infrastructure to support AI-driven fertility solutions. I work atthe intersection of data engineering and machine learning operations, ensuringseamless deployment, monitoring, and automation of ML models in production.My responsibilities include data preprocessing, model versioning, CI/CD for MLworkflows, and enabling reproducibility, reliability, and scalability of AI systemsused in IVF clinics.\r\n",
      "Optum\r\n",
      "Data Engineer\r\n",
      "October 2023 - April 2025 (1 year 7 months)\r\n",
      "Pune, Maharashtra, India\r\n",
      "• Designed and implemented data pipelines for processing large-scalehealthcare datasets, ensuring data integrity and compliance.• Leveraged AI and machine learning models to derive actionable insights,improving patient outcomes.• Utilized big data technologies like Hadoop and Spark for real-time and batchprocessing.\r\n",
      "Innova Solutions\r\n",
      "Data Scientist\r\n",
      "March 2022 - September 2023 (1 year 7 months)\r\n",
      "Pune, Maharashtra, India\r\n",
      "I designed and developed AI-driven models to extract actionable insights fromcomplex healthcare datasets, improving patient outcomes and operationalefficiency. By applying machine learning and deep learning techniques, Ipredicted disease trends, optimized treatment plans, and enhanced clinicaldecision-making. Collaborating with cross-functional teams, including cliniciansand business stakeholders, I translated data-driven insights into practicalhealthcare solutions.\r\n",
      "BioEnable Technologies Pvt Ltd\r\n",
      "Page 2 of 3\r\n",
      "Data Scientist\r\n",
      "August 2019 - March 2022 (2 years 8 months)\r\n",
      "Pune, Maharashtra, India\r\n",
      "• Designed and trained machine learning models for object detection, objectsegmentation and facial recognition.• Utilized tools like OpenCV, TensorFlow, and PyTorch to optimize algorithmsfor performance.\r\n",
      "zCon Solutions\r\n",
      "Data Engineer | Data Scientist\r\n",
      "October 2018 - July 2019 (10 months)\r\n",
      "Pune Area, India\r\n",
      "Emergys\r\n",
      "Data Engineer | Data Scientist\r\n",
      "August 2017 - July 2018 (1 year)\r\n",
      "Pune,India\r\n",
      "Education\r\n",
      "Pune Vidhyarthi Griha's College of Engineering and TechnologyPune\r\n",
      "Bachelor of Engineering (B.E.), Computer Engineering · (2013 - 2017)\n",
      "\n",
      "Question: how many years experience does Tushar Wagh has ?\n",
      "\n",
      "Answer concisely using the context above. Tushar Wagh has approximately 5 years and 3 months of experience in total based on the information provided in his LinkedIn profile. This includes his current role as a Data and MLOps Engineer at Fairtility, and his previous roles as a Data Engineer at Optum, a Data Scientist at Innova Solutions and BioEnable Technologies, and shorter tenures at zCon Solutions and Emergys.\n"
     ]
    }
   ],
   "source": [
    "# 8️⃣ Example usage\n",
    "query = \"how many years experience does Tushar Wagh has ?\"\n",
    "print(generate_response(query))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
